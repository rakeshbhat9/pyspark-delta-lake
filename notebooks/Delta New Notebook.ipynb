{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee6a63b-a782-4fb7-8b2d-24a8e00413d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56714668-fe47-4077-a53f-e8ce8cf20ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Config for notebook - optional\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "066ca0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d61c2f75-e9e8-4b86-8302-854c157f10db;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 234ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d61c2f75-e9e8-4b86-8302-854c157f10db\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "21/12/24 12:02:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d0dc9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+\n",
      "|      date|store|item|sales|\n",
      "+----------+-----+----+-----+\n",
      "|2013-01-01|    1|   1|   13|\n",
      "|2013-01-02|    1|   1|   11|\n",
      "|2013-01-03|    1|   1|   14|\n",
      "|2013-01-04|    1|   1|   13|\n",
      "|2013-01-05|    1|   1|   10|\n",
      "+----------+-----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data in to dataframe\n",
    "\n",
    "df = spark.read.option(\"header\",True).csv('../data/train.csv')\n",
    "\n",
    "df.count()\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96d17afa-8710-4c83-8663-ee0a190368f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "958000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart kernel and readin data from delta lake\n",
    "\n",
    "delta_df = spark.read.format(\"delta\").load(\"../lake\")\n",
    "\n",
    "type(delta_df)\n",
    "\n",
    "delta_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c9f831-dc09-43f4-b455-c10ea00f2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check Delta table's meta data\n",
    "deltaTable = DeltaTable.forPath(spark, \"../lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dce0454-f0aa-4385-8f88-f7edb93ad0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/24 12:05:14 WARN DeltaHistoryManager: Found Delta commit 2 with a timestamp 1640106233377 which is greater than the next commit timestamp 1640106233377.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-12-21 17:03:53.378</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Append', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numOutputRows': '45000', 'numOutputBytes': '198755', 'numFiles': '1'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-12-21 17:03:53.377</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '913000', 'numOutputBytes': '1773875', 'numFiles': '4'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-21 17:03:53.376</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '913000', 'numOutputBytes': '1776421', 'numFiles': '4'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-21 17:03:53.375</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'ErrorIfExists', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numOutputRows': '913000', 'numOutputBytes': '1773875', 'numFiles': '4'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName operation  \\\n",
       "0        3 2021-12-21 17:03:53.378   None     None     WRITE   \n",
       "1        2 2021-12-21 17:03:53.377   None     None     WRITE   \n",
       "2        1 2021-12-21 17:03:53.376   None     None     WRITE   \n",
       "3        0 2021-12-21 17:03:53.375   None     None     WRITE   \n",
       "\n",
       "                              operationParameters   job notebook clusterId  \\\n",
       "0         {'mode': 'Append', 'partitionBy': '[]'}  None     None      None   \n",
       "1      {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n",
       "2      {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n",
       "3  {'mode': 'ErrorIfExists', 'partitionBy': '[]'}  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          2.0           None           True   \n",
       "1          1.0           None          False   \n",
       "2          0.0           None          False   \n",
       "3          NaN           None           True   \n",
       "\n",
       "                                                            operationMetrics  \\\n",
       "0    {'numOutputRows': '45000', 'numOutputBytes': '198755', 'numFiles': '1'}   \n",
       "1  {'numOutputRows': '913000', 'numOutputBytes': '1773875', 'numFiles': '4'}   \n",
       "2  {'numOutputRows': '913000', 'numOutputBytes': '1776421', 'numFiles': '4'}   \n",
       "3  {'numOutputRows': '913000', 'numOutputBytes': '1773875', 'numFiles': '4'}   \n",
       "\n",
       "  userMetadata engineInfo  \n",
       "0         None       None  \n",
       "1         None       None  \n",
       "2         None       None  \n",
       "3         None       None  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.history().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c528c75b-4def-4919-8616-83cc3d27d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_delta_log\n",
      "part-00000-457fb9cf-fd52-423d-9af9-c2922d0c85fe-c000.snappy.parquet\n",
      "part-00000-59d2f1cc-14fb-43a9-a949-8c595261cc50-c000.snappy.parquet\n",
      "part-00000-bb692ea7-87a1-4af0-b7f3-95189f10bdcf-c000.snappy.parquet\n",
      "part-00000-f8a68c06-8d60-40a7-9b02-7286f33a2c03-c000.snappy.parquet\n",
      "part-00001-225c2da3-6d10-4561-a7eb-66d51c4e0720-c000.snappy.parquet\n",
      "part-00001-a6373976-cac8-45d4-ab3d-120c40bd7222-c000.snappy.parquet\n",
      "part-00001-e56cef05-cf8a-4784-b764-260aa709f1e0-c000.snappy.parquet\n",
      "part-00002-2236d358-9a8e-4d96-bb24-38caecae18d6-c000.snappy.parquet\n",
      "part-00002-712810b1-3c68-4e17-9536-246c20c10d4b-c000.snappy.parquet\n",
      "part-00002-f1bf05c6-8570-4fda-84f7-e33d320e910e-c000.snappy.parquet\n",
      "part-00003-376097a8-e45b-4d0e-a3fd-7940325b6c65-c000.snappy.parquet\n",
      "part-00003-7557dfdc-4c47-42d3-9801-4c80cafa6c21-c000.snappy.parquet\n",
      "part-00003-e92961a8-b7a4-4463-ae9a-6f9e146cbc80-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Below are all parquet files related to above changes\n",
    "!ls ../lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dcd82a2-bc94-4636-8ea5-a326185faa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(913000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date store item sales\n",
       "0  2013-01-01     1    1    13\n",
       "1  2013-01-02     1    1    11\n",
       "2  2013-01-03     1    1    14\n",
       "3  2013-01-04     1    1    13\n",
       "4  2013-01-05     1    1    10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart kernel and now we can read data as of version we are interested in.\n",
    "old_table = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"../lake\")\n",
    "print((old_table.count(), len(old_table.columns)))\n",
    "old_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f313b66a-6624-42a5-8c39-42eddf8e7662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(913000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "      <th>col_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date store item sales  col_n\n",
       "0  2013-01-01     1    1    13    2.0\n",
       "1  2013-01-02     1    1    11    2.0\n",
       "2  2013-01-03     1    1    14    2.0\n",
       "3  2013-01-04     1    1    13    2.0\n",
       "4  2013-01-05     1    1    10    2.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_table = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"../lake\")\n",
    "print((new_table.count(), len(new_table.columns)))\n",
    "new_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3cab96-5909-468a-8b8b-584de87b5897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
